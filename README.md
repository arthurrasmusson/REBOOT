# REBOOT  
**R**Eboot **E**xtends **B**inary reverseâ€‘engineering via **O**verâ€‘RDMAâ€‘Pagedâ€‘Attention & **O**penAIâ€‘API **T**ransforms  

---

## 0â€¯.â€¯AI for RE

* **What?** &nbsp;A singleâ€‘file Python 3 CLI (`openâ€‘refactor.py`) that can
  *harvest* arbitrarily large codebasesâ€”including embedded binariesâ€”into a
  tokenâ€‘friendly text bundle **and** push it (plus your system prompt) to any
  OpenAIâ€‘compatible Chat Completions endpoint.

* **Why?** &nbsp;Feed a *longâ€‘context* LLM (gptâ€‘4oâ€‘long, homeâ€‘grown 10â€¯Mâ€‘token
  model, etc.) everything it needs for a **oneâ€‘shot refactor, audit, or reverse
  engineering pass**â€”no retrieval plugin required.

* **Who?** &nbsp;Teams with a DGX, H100 pods, or an Azure â€œgigaâ€‘contextâ€ model
  subscription who want to modernise or unmangle legacy monorepos *fast*.

---

## 1â€¯.â€¯Features at a Glance

| Category | Highlights |
|----------|------------|
| **Harvest** | Multiâ€‘root crawl, VCSâ€‘dirs ignored, text streamed lineâ€‘wise, binary options (omit / Baseâ€‘64 / disassembly). |
| **Tokenâ€‘aware** | Counts tokens (exact with `tiktoken`, heuristic fallback), prints a **budget table**, aborts if you exceed the modelâ€™s context (unless `--allow-overflow`). |
| **Pushâ€‘button upload** | Reads system prompt from file, sends **system** + **user** messages to any OpenAI endpoint; streams replies live. |
| **Memoryâ€‘safe** | No file read fully into RAM unless <â€¯10â€¯MB *and* needed for token precision. |
| **Portable** | Pure stdâ€‘lib unless you enable extras (`openai`, `tiktoken`, `objdump`, `Ghidra`). Works on Linux, macOS, Windows. |

---

## 2â€¯.â€¯Prerequisites

| Purpose | Package | Install |
|---------|---------|---------|
| ğŸ—£ï¸Â Chat API | `openai>=1.6` | `pip install --upgrade openai` |
| ğŸ”¢Â Accurate token count | `tiktoken>=0.5` *(optional)* | `pip install tiktoken` |
| ğŸ› ï¸Â Disassembly | `objdump` *(GNUÂ binutils)* | system package manager |
| ğŸ”Â Deep binary analysis | **Ghidra** *(optional)* | download & set `GHIDRA_INSTALL_DIR` |

PythonÂ â‰¥â€¯3.8 is required.

---

## 3â€¯.â€¯Installation

```bash
git clone https://github.com/arthurrasmusson/REBOOT.git
cd REBOOT
python -m pip install -r requirements.txt    # pulls openai + tiktoken
chmod +x open-refactor.py                    # or: ln -s â€¦/open-refactor.py ~/bin/reboot
````

---

## 4â€¯.â€¯Anatomy of `open-refactor.py`

```text
â–ˆâ–ˆ HARVEST
   â”œâ”€ process_root()
   â”‚   â”œâ”€ directory listing block
   â”‚   â””â”€ perâ€‘file blocks (text vs binary)
   â””â”€ output â†’ open-refactor-input-tokens.txt   (UTFâ€‘8)

â–ˆâ–ˆ TOKEN REPORT
   â”œâ”€ estimate_file_tokens()   â€“ precise if <10â€¯MB & tiktoken, else heuristic
   â”œâ”€ context window lookup    â€“ table in script (edit for custom IDs)
   â””â”€ abort/warn if overflow   â€“ unless --allow-overflow

â–ˆâ–ˆ UPLOAD (optional)
   â”œâ”€ Reads system prompt (--path-to-refactor-prompt)
   â”œâ”€ Configures endpoint (--openai-base-url)
   â””â”€ Streams reply or waits (default stream, --openai-no-stream for full)
```

### Key Data Flow

```mermaid
graph TD
A[Source roots] -->|walk| B(open-refactor.py)
B -->|text| C(open-refactor-input-tokens.txt)
C -->|user msg| D(OpenAI endpoint)
E[Prompt.txt] -->|system msg| D
D -->|completion| F[Terminal stream / file]
```

*(Diagram rendered by GitHubâ€™s Mermaid)*

---

## 5â€¯.â€¯Commandâ€‘line Cheatâ€‘Sheet

```bash
# Minimal harvest
open-refactor.py --root-source-path PATH [...]

# Include objdump summaries of binaries
open-refactor.py [...] --collect-binary-disassembly

# Baseâ€‘64 embed binaries (inflates size!)
open-refactor.py [...] --collect-binaries-full

# Produce file AND push to model
export OPENAI_API_KEY=sk-...
open-refactor.py [...] \
    --path-to-refactor-prompt prompt.md \
    --openai-model gpt-4o-long

# Offline / airâ€‘gapped â€“ skip upload
open-refactor.py [...] --skip-upload

# Ignore overflow warning (e.g. for 10â€¯Mâ€‘token custom model)
open-refactor.py [...] --allow-overflow

# Control expected completion budget
open-refactor.py [...] --max-output-tokens 2048
```

---

## 6â€¯.â€¯Token Budget Output

At every run youâ€™ll see something like:

```text
[Token budget]  System:  184   +  User: 9,881,462  +  Overhead: 8
  = INPUT 9,881,654  |  Planned output 4,096  =>  TOTAL 9,885,750 tokens
[!] WARNING: total tokens (9,885,750) exceed gpt-4o context (128,000).
```

Actions:

* **Reduce input** â€“ split repo, remove thirdâ€‘party libs, omit binaries.
* **Pick bigger model** â€“ e.g. `gpt-4o-long`.
* **`--allow-overflow`** â€“ if your endpoint actually supports more.

---

## 7â€¯.â€¯Binary Handling Matrix

| Flag combo                     | Effect in output                                  | When to use                                                          |
| ------------------------------ | ------------------------------------------------- | -------------------------------------------------------------------- |
| *(default)*                    | `"[note that this file was omitted]"` placeholder | You only need symbols/file names.                                    |
| `--collect-binary-disassembly` | `objdump -d` (first 20â€¯kB) or Ghidra report       | You need inline asm to refactor adjacent C/C++ code.                 |
| `--collect-binaries-full`      | Baseâ€‘64 blob                                      | Rare; maybe youâ€™ll pipe bytes into an LLM able to decompile raw ELF. |

---

## 8â€¯.â€¯Endâ€‘toâ€‘End Example

```bash
# Harvest two NVIDIA repos, get disassembly, push to Azure OpenAI,
# stream model output to a markdown patch file.

open-refactor.py \
  --root-source-path ~/src/tensorrt-llm \
  --root-source-path ~/src/libnvinfer \
  --collect-binary-disassembly \
  --output /tmp/trt_bundle.txt \
  --path-to-refactor-prompt prompts/upgrade_cuda12.md \
  --openai-base-url https://mycompany.openai.azure.com/openai/deployments/v4 \
  --openai-model gpt-4o-long \
  --max-output-tokens 8192 \
  | tee /tmp/patch.md
```

---

## 9â€¯.â€¯Security Considerations

* **Secrets** â€“ The harvest is literal. Add `.env`, `*.pem`, etc. to a
  `.rebootignore` *(feature planned)* or run a scrub pass before upload.
* **Licensing** â€“ Thirdâ€‘party source may not allow uploading to OpenAI; audit data.
* **Cost** â€“ 10â€¯M tokens â‡¢ \$\$\$.  Verify with `--skip-upload` first.

---

## 10â€¯.â€¯Limitations / Roadmap

* **No incremental diff** â€“ always fullâ€‘repo upload. Chunking/RAG mode TBD.
* **Ghidra headless** â€“ The script currently calls `objdump` only; a richer
  `--collect-binary-disassembly-ghidra` flag is planned.
* **Windows objdump** â€“ require MSYS or LLVM `llvm-objdump`.

---

## 11â€¯.â€¯Contributing

1. Fork â†’ Branch â†’ PR.
2. Run `python -m pip install -r requirements-dev.txt && make lint test`.
3. Ensure new features have docstrings + README update.

---

## 12â€¯.â€¯License

AGPLv3

---

## 13â€¯.â€¯Reverse Acronym (because why not?)

> **R**Eboot **E**xtends **B**inary reverseâ€‘engineering via **O**verâ€‘RDMAâ€‘Pagedâ€‘Attention & **O**penAIâ€‘API **T**ransforms

